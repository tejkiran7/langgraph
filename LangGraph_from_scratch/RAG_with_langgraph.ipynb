{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TEJKIRAN\\anaconda3\\envs\\langgraph\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=DirectoryLoader(\"../data\",glob=\"./*.txt\",loader_cls=TextLoader)\n",
    "docs=loader.load()\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "new_docs = text_splitter.split_documents(documents=docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(new_docs, embeddings)\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEJKIRAN\\AppData\\Local\\Temp\\ipykernel_8092\\1985905.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '..\\\\data\\\\llama3.txt'}\n",
      "Alongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in\n",
      "page_content='Alongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in' metadata={'source': '..\\\\data\\\\llama3.txt'}\n",
      "page_content='by Meta AI starting in February 2023.[2][3] The latest version is Llama 3 released in April' metadata={'source': '..\\\\data\\\\llama3.txt'}\n",
      "page_content='Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.[7]' metadata={'source': '..\\\\data\\\\llama3.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"what is meta llama3?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(docs[0].metadata)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(AgentState):\n",
    "    \n",
    "    message=AgentState[\"messages\"]\n",
    "    \n",
    "    question=message[0]\n",
    "    \n",
    "    complete_prompt=\"Your task is to provide only the brief answer based on the user query. \\\n",
    "        Don't include too much reasoning. Following is the user query: \" + question\n",
    "    \n",
    "    response = llm.invoke(complete_prompt)\n",
    "    \n",
    "    AgentState['messages'].append(response.content) # appending LLM call response to the AgentState\n",
    "    \n",
    "    # print(AgentState)\n",
    "    \n",
    "    return AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(AgentState):\n",
    "    messages = AgentState['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "from langgraph.graph import Graph\n",
    "workflow4 = Graph()\n",
    "workflow4.add_node(\"LLM\", function_1)\n",
    "workflow4.add_node(\"RAGtool\", function_2)\n",
    "workflow4.add_edge('LLM', 'RAGtool')\n",
    "workflow4.set_entry_point(\"LLM\")\n",
    "workflow4.set_finish_point(\"RAGtool\")\n",
    "app4 = workflow4.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app4.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [\"How many parameters does GPT-3 have?\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'LLM':\n",
      "---\n",
      "{'messages': ['How many parameters does GPT-3 have?', '175 billion']}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'RAGtool':\n",
      "---\n",
      "175B\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app4.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With two tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader2=DirectoryLoader(\"../data2\",glob=\"./*.txt\",loader_cls=TextLoader)\n",
    "docs2=loader2.load()\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "new_docs2 = text_splitter.split_documents(documents=docs2)\n",
    "doc_strings2 = [doc.page_content for doc in new_docs2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = Chroma.from_documents(new_docs2, embeddings)\n",
    "retriever2 = db2.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '..\\\\data2\\\\indian_economy.txt'}\n",
      "India’s industrial growth has outpaced expectations, raising hopes that the economy will avoid a\n",
      "page_content='India’s industrial growth has outpaced expectations, raising hopes that the economy will avoid a' metadata={'source': '..\\\\data2\\\\indian_economy.txt'}\n",
      "page_content='India’s strong export sectors—including textiles, IT services, and automotive manufacturing—had' metadata={'source': '..\\\\data2\\\\indian_economy.txt'}\n",
      "page_content='India’s last eight years of GDP (hypothetical data):' metadata={'source': '..\\\\data2\\\\indian_economy.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about India's Industrial Growth?\"\n",
    "docs = retriever2.get_relevant_documents(query)\n",
    "print(docs[0].metadata)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TypedDict:<br>\n",
    "TypedDict is a special type that allows you to define a dictionary in Python where each key has a specific type. It’s useful when you want to enforce type checks on the keys and values of a dictionary.<br>\n",
    "\n",
    "Annotated:<br>\n",
    "Annotated is used to add metadata or constraints to a type. It’s typically used for adding additional information, like constraints or descriptions, to the type.<br>\n",
    "\n",
    "Sequence[BaseMessage]:<br>\n",
    "Sequence is a generic type from Python's typing module that represents a list-like structure (such as a list or tuple) where the order matters, and you can access elements by their position.<br>\n",
    "\n",
    "BaseMessage:<br>\n",
    "Basemessage: This assumes you're working with the LangChain framework, and BaseMessage is part of its core messaging system. It is typically used when handling or processing messages in LangChain's workflows<br>\n",
    "\n",
    "operator.add:<br>\n",
    "operator.add is a function from Python’s operator module that performs addition (+) on its arguments. In this context, operator.add is being used as metadata for the Annotated type, likely suggesting that the sequence of BaseMessage objects is meant to be \"added\" or concatenated with another sequence at some point. It’s an indicator of how the messages field will be treated in the logic of your program.<br>\n",
    "\n",
    "TopicSelectionParser: A Pydantic model that defines two fields (Topic and Reasoning), both of which are required to be strings.\n",
    "\n",
    "Field: Adds metadata, like descriptions, and can also be used for validation and constraints.\n",
    "\n",
    "Pydantic’s Role: Ensures that the data matches the expected structure and types, raising validation errors if the data is invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain.prompts import PromptTemplate    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    # The 'messages' field should be a sequence of strings, and we annotate it with 'operator.add'\n",
    "    # This implies we might want to \"add\" new messages to the sequence later\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "class TopicSelectionParser(BaseModel):\n",
    "    Topic: str = Field(description='Selected Topic')\n",
    "    Reasoning: str = Field(description='Reasoning behind topic selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=TopicSelectionParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    message=state[\"messages\"]\n",
    "    question=message[-1]\n",
    "    print(question)\n",
    "    \n",
    "    template=\"\"\"\n",
    "    Your task is to classify the given user query into one of the following categories: [India, Not Related]. \n",
    "    If the user question is related to indian economy classify it as \"India\" else \"Not Related\"\n",
    "    Only respond with the category name and nothing else.\n",
    "\n",
    "    User query: {question}\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=template,\n",
    "                                    input_variables=[question],\n",
    "                                    partial_variables={\n",
    "                                        \"format_instructions\" : parser.get_format_instructions()                                    }\n",
    "                                    )\n",
    "    chain =  prompt | llm | parser\n",
    "    \n",
    "    response = chain.invoke({\"question\":question,\"format_instructions\" : parser.get_format_instructions() })\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    return {\"messages\": [response.Topic]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state):\n",
    "    print('-> Router ->')\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    print(last_message)\n",
    "    if 'India' in last_message:\n",
    "        return 'RAG Call'\n",
    "    else:\n",
    "        return 'LLM Call'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(state):\n",
    "    print('-> Calling RAG ->')\n",
    "    messages = state['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "    print(question)\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    print(prompt)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return  {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3(state):\n",
    "    print('-> Calling LLM ->')\n",
    "\n",
    "    messages = state['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    # Normal LLM call\n",
    "    complete_query = \"Anwer the follow question with your knowledge of the real world. Following is the user question: \" + question\n",
    "    response = llm.invoke(complete_query)\n",
    "    return {\"messages\": [response.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.AgentState"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,END\n",
    "\n",
    "workflow5 = StateGraph(AgentState) ### StateGraph with AgentState\n",
    "\n",
    "\n",
    "workflow5.add_node(\"agent\", function_1)\n",
    "\n",
    "workflow5.add_node(\"RAG\", function_2)\n",
    "\n",
    "workflow5.add_node(\"LLM\", function_3)\n",
    "\n",
    "\n",
    "workflow5.set_entry_point(\"agent\")\n",
    "\n",
    "workflow5.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    \n",
    "    \n",
    "    router,\n",
    "    {\n",
    "        \"RAG Call\": \"RAG\",\n",
    "        \"LLM Call\": \"LLM\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow5.add_edge(\"RAG\",END)\n",
    "\n",
    "\n",
    "workflow5.add_edge(\"LLM\",END)\n",
    "\n",
    "\n",
    "app5=workflow5.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [\"Tell me about India's Industrial Growth\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about India's Industrial Growth\n",
      "Topic='India' Reasoning=\"The user is asking about India's Industrial Growth, which is related to indian economy.\"\n",
      "-> Router ->\n",
      "India\n",
      "-> Calling RAG ->\n",
      "Tell me about India's Industrial Growth\n",
      "input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n    {context}\\n\\n    Question: {question}\\n    '))]\n"
     ]
    }
   ],
   "source": [
    "output = app5.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [\"Tell me about India's Industrial Growth\",\n",
       "  'India',\n",
       "  'India’s industrial growth has outpaced expectations']}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [\"Who build Tajmahal?\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who build Tajmahal?\n",
      "Topic='Not Related' Reasoning='The user query is not related to Indian economy, it is about the history of Tajmahal'\n",
      "-> Router ->\n",
      "Not Related\n",
      "-> Calling LLM ->\n"
     ]
    }
   ],
   "source": [
    "output = app5.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': ['Who build Tajmahal?', 'Not Related', 'Shah Jahan']}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
